{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607",
      "metadata": {
        "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607"
      },
      "source": [
        "# Movie Review Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b8a036",
      "metadata": {},
      "source": [
        "## 0. Project Overview\n",
        "\n",
        "This project demonstrates the application of Natural Language Processing (NLP) techniques to perform sentiment analysis on movie reviews. We will train a supervised machine learning model to predict whether a given movie review is positive or negative.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We utilize the movie review dataset from the Natural Language Toolkit (NLTK) library. The dataset consists of:\n",
        "\n",
        "- 1000 positive reviews\n",
        "- 1000 negative reviews\n",
        "\n",
        "This balanced collection provides an excellent foundation for training our binary classification model.\n",
        "\n",
        "## Methodology\n",
        "\n",
        "Our approach involves:\n",
        "\n",
        "1. Data preprocessing\n",
        "2. Feature extraction using NLP techniques\n",
        "3. Model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
        "outputId": "46c1d8f9-5493-4df8-e40f-b62cf7bfbbfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     /Users/nivyni/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/nivyni/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"movie_reviews\")\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('punkt')\n",
        "stop = stopwords.words('english')\n",
        "import string\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Nmt1nVaNpkKy",
      "metadata": {
        "id": "Nmt1nVaNpkKy"
      },
      "outputs": [],
      "source": [
        "negative_fileids = movie_reviews.fileids('neg')\n",
        "positive_fileids = movie_reviews.fileids('pos')\n",
        "\n",
        "pos_document = [(' '.join(movie_reviews.words(file_id)),category) for file_id in movie_reviews.fileids() for category in movie_reviews.categories(file_id) if category == 'pos']\n",
        "neg_document = [(' '.join(movie_reviews.words(file_id)),category) for file_id in movie_reviews.fileids() for category in movie_reviews.categories(file_id) if category == 'neg']\n",
        "\n",
        "# List of postive and negative reviews\n",
        "pos_list = [pos[0] for pos in pos_document]\n",
        "neg_list = [neg[0] for neg in neg_document]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "zFkLGJ5p118y",
      "metadata": {
        "id": "zFkLGJ5p118y"
      },
      "outputs": [],
      "source": [
        "reviews_list = pos_document + neg_document\n",
        "reviews_df = pd.DataFrame(reviews_list, columns=['reviews','label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1a09323b",
      "metadata": {
        "id": "1a09323b"
      },
      "outputs": [],
      "source": [
        "reviews_X = reviews_df['reviews'].to_frame()\n",
        "reviews_y = reviews_df['label'].to_frame()\n",
        "reviews_X_dev, reviews_X_test,reviews_y_dev,reviews_y_test = train_test_split(reviews_X,reviews_y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d",
      "metadata": {
        "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d"
      },
      "source": [
        "## 1. Data preprocessing\n",
        "\n",
        "Remove `#` symbol, hyperlinks, stop words & punctuations from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5dfcdda0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove hashtags from reviews\n",
        "reviews_X_dev['reviews'] = reviews_X_dev['reviews'].str.replace(r'#\\w+', '', regex=True)\n",
        "reviews_X_test['reviews'] = reviews_X_test['reviews'].str.replace(r'#\\w+', '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ff5a7411-df49-427b-adef-5e8e63224db0",
      "metadata": {
        "id": "ff5a7411-df49-427b-adef-5e8e63224db0"
      },
      "outputs": [],
      "source": [
        "# Remove URLs from reviews\n",
        "url_pattern = r'(https?://\\S+|www\\.\\S+)'\n",
        "reviews_X_dev['reviews'] = reviews_X_dev['reviews'].str.replace(url_pattern, '', regex=True)\n",
        "reviews_X_test['reviews'] = reviews_X_test['reviews'].str.replace(url_pattern, '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "961d73fd-a662-46f2-85a2-83bf6b978189",
      "metadata": {
        "id": "961d73fd-a662-46f2-85a2-83bf6b978189"
      },
      "outputs": [],
      "source": [
        "# Remove stop words from reviews\n",
        "stop = set(stopwords.words('english'))\n",
        "for i in stop:\n",
        "    reviews_X_dev['reviews'] = reviews_X_dev['reviews'].str.replace(r'\\b{}\\b'.format(i), '', regex=True)\n",
        "    reviews_X_test['reviews'] = reviews_X_test['reviews'].str.replace(r'\\b{}\\b'.format(i), '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e",
      "metadata": {
        "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e"
      },
      "outputs": [],
      "source": [
        "# Remove punctuation from reviews\n",
        "punctuation_pattern = rf'[{re.escape(string.punctuation)}]'\n",
        "reviews_X_dev['reviews'] = reviews_X_dev['reviews'].str.replace(punctuation_pattern, '', regex=True)\n",
        "reviews_X_test['reviews'] = reviews_X_test['reviews'].str.replace(punctuation_pattern, '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f1af18-0c07-4ffb-994e-daead4740a53",
      "metadata": {
        "id": "b2f1af18-0c07-4ffb-994e-daead4740a53"
      },
      "source": [
        "### Apply Stemming to Development and Test Datasets\n",
        "\n",
        "In this step, we'll apply the Porter stemming algorithm to our review texts. Stemming reduces words to their root form, which can help in reducing vocabulary size and improving text analysis.\n",
        "\n",
        "We'll use the Porter stemmer from the NLTK library and apply it to both our development and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c84a52f6-a62a-4033-8d1d-239ff6904248",
      "metadata": {
        "id": "c84a52f6-a62a-4033-8d1d-239ff6904248"
      },
      "outputs": [],
      "source": [
        "def stem_sentence(sentence):\n",
        "    stemmer = PorterStemmer()\n",
        "    return ' '.join(stemmer.stem(word) for word in word_tokenize(sentence))\n",
        "\n",
        "# Apply stemming to development and test datasets\n",
        "reviews_X_dev['reviews_stem'] = reviews_X_dev['reviews'].apply(stem_sentence)\n",
        "reviews_X_test['reviews_stem'] = reviews_X_test['reviews'].apply(stem_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687e23ef-dafd-4183-b2f1-86089e281dd8",
      "metadata": {
        "id": "687e23ef-dafd-4183-b2f1-86089e281dd8"
      },
      "source": [
        "## 2. Feature Extraction\n",
        "\n",
        "After preprocessing our text data, we'll now perform feature extraction using the Bag of Words model. This step converts our text data into a numerical format that machine learning algorithms can process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef",
      "metadata": {
        "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef"
      },
      "source": [
        "### Logic Behind Bag of Words Feature Extraction\n",
        "\n",
        "The Bag of Words (BoW) model is a simple yet powerful technique for representing text data in machine learning. Here's the underlying logic:\n",
        "\n",
        "1. **Vocabulary Creation**:\n",
        "   - The `fit` method of CountVectorizer scans all documents in the development set.\n",
        "   - It creates a vocabulary of unique words across all documents.\n",
        "   - Each word in this vocabulary becomes a feature.\n",
        "\n",
        "2. **Vector Representation**:\n",
        "   - Each document (review in our case) is represented as a vector.\n",
        "   - The length of this vector is equal to the size of the vocabulary.\n",
        "   - Each element in the vector represents a word count.\n",
        "\n",
        "3. **Sparse Matrix**:\n",
        "   - The result is a sparse matrix where:\n",
        "     - Rows represent documents (reviews)\n",
        "     - Columns represent words in the vocabulary\n",
        "     - Cell values represent the count of a word in a document\n",
        "\n",
        "4. **Feature Matrix Creation**:\n",
        "   - `fit_transform` on the development set:\n",
        "     - Creates the vocabulary\n",
        "     - Transforms the development set into a feature matrix\n",
        "   - `transform` on the test set:\n",
        "     - Uses the vocabulary created from the development set\n",
        "     - Ensures consistency in features between development and test sets\n",
        "\n",
        "5. **Importance of Separate Handling**:\n",
        "   - We only `fit` on the development set to prevent data leakage.\n",
        "   - The test set is transformed using the vocabulary from the development set, simulating real-world scenarios where we'd apply our model to unseen data.\n",
        "\n",
        "This approach turns our text data into a numerical format that machine learning models can process, while maintaining the frequency information of words in each document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a",
      "metadata": {
        "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a"
      },
      "outputs": [],
      "source": [
        "vector = CountVectorizer()\n",
        "dev_X = vector.fit_transform(reviews_X_dev['reviews_stem'])\n",
        "test_X = vector.transform(reviews_X_test['reviews_stem'])\n",
        "dev_y = reviews_y_dev['label']\n",
        "test_y = reviews_y_test['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986bd40f",
      "metadata": {},
      "source": [
        "### Logic Behind TF-IDF Vectorization\n",
        "\n",
        "1. **TF-IDF Concept**:\n",
        "   - TF-IDF stands for Term Frequency-Inverse Document Frequency.\n",
        "   - It evaluates the importance of a word to a document in a collection or corpus.\n",
        "\n",
        "2. **Term Frequency (TF)**:\n",
        "   - Measures how frequently a term occurs in a document.\n",
        "   - TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "\n",
        "3. **Inverse Document Frequency (IDF)**:\n",
        "   - Measures how important a term is across the entire corpus.\n",
        "   - IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "\n",
        "4. **TF-IDF Score**:\n",
        "   - TF-IDF = TF * IDF\n",
        "   - Increases with term frequency in a document and rarity across the corpus.\n",
        "\n",
        "5. **Vectorization Process**:\n",
        "   - `fit_transform` on development set: Computes vocabulary, IDF values, and transforms documents.\n",
        "   - `transform` on test set: Uses vocabulary and IDF from development set to transform test documents.\n",
        "\n",
        "6. **Advantages**:\n",
        "   - Reduces impact of frequent but less informative words.\n",
        "   - Emphasizes rare terms that are important in specific documents.\n",
        "\n",
        "7. **Stop Words Removal**:\n",
        "   - `stop_words='english'` automatically removes common English stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8c1548",
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vector = TfidfVectorizer(stop_words = 'english')\n",
        "dev_X_tfidf = tfidf_vector.fit_transform(reviews_X_dev['reviews_stem'])\n",
        "test_X_tfidf = tfidf_vector.transform(reviews_X_test['reviews_stem'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f2f20da",
      "metadata": {},
      "source": [
        "## 3. Model training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e",
      "metadata": {
        "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3433a6b0-408d-462e-9072-3495b21bc97b",
      "metadata": {
        "id": "3433a6b0-408d-462e-9072-3495b21bc97b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nivyni/opt/anaconda3/envs/dl4995/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "lr = LogisticRegression().fit(dev_X, dev_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce",
      "metadata": {
        "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce"
      },
      "outputs": [],
      "source": [
        "lr_tfidf = LogisticRegression().fit(dev_X_tfidf, dev_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7ca644f9",
      "metadata": {
        "id": "7ca644f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for Bag of Words Model:  0.83\n",
            "Score for TF IDF Model:  0.8375\n"
          ]
        }
      ],
      "source": [
        "print('Score for Bag of Words Model:  {}'.format(lr.score(test_X, test_y)))\n",
        "print('Score for TF IDF Model:  {}'.format(lr_tfidf.score(test_X_tfidf,test_y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61295632",
      "metadata": {},
      "source": [
        "### Comparison\n",
        "\n",
        "The TF-IDF Model has a slightly higher score. The reasons for this are:\n",
        "\n",
        "1. **Vectorization Approach**:\n",
        "   - Bag of Words: Simply vectorizes the text.\n",
        "   - TF-IDF: Distributes weight based on term importance.\n",
        "\n",
        "2. **Weight Distribution**:\n",
        "   - TF-IDF assigns:\n",
        "     - More weight to important terms\n",
        "     - Less weight to common (less important) terms\n",
        "\n",
        "3. **Learning Efficiency**:\n",
        "   - TF-IDF model can learn more effectively from the training set.\n",
        "\n",
        "4. **Prediction Quality**:\n",
        "   - Due to its weighted approach, TF-IDF tends to make better predictions on the test set.\n",
        "\n",
        "In summary, the TF-IDF model's ability to distinguish between important and common terms allows it to capture more nuanced information from the text, leading to improved performance compared to the simpler Bag of Words approach."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('dl4995')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed0d11e2e60a2a8078b28d52043bcbf35c575a83cf1567a7bea5930e71c75104"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
